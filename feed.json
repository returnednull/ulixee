{
    "version": "https://jsonfeed.org/version/1",
    "title": "Ulixee Blog",
    "home_page_url": "https://ulixee.org/",
    "feed_url": "https://ulixee.org/feed.json",
    "description": "A blog about scraping, features and experiences developing Ulixee",
    "items": [
        {
            "id": "https://ulixee.org/blog/chromium-chrome",
            "content_html": "\nWe're moving our underlying engine from Chromium to Chrome in the coming weeks. \n\n## Why?\nThere are a few reasons we decided to go this direction:\n1. Chrome is the actual browser being used in the wild by consumers.\n2. Chrome has increasingly diverged from Chromium. In our DoubleAgent testing, we're seeing Chrome 85-89 steadily diverge features. This makes it harder and harder to emulate Chrome when using Chromium as the engine.\n3. Chrome has certain features that aren't in Chromium that will be nearly impossible to emulate (x-headers to Google sites, Widevine, etc). In theory, you could use DRM as a way to weed out Chromium users masking themselves as Chrome users.\n\n## Chrome Version-Specific Installers \nThis switch was somewhat challenging, primarily because the Chrome team doesn't openly publish versions of Chrome that stay on the version you want them on. Even on Ubuntu, if you install the .deb release, it will install an apt updater, and if you're not careful, your engine will swap out underneath you.\n\nOur first task was to go out and find stable Chrome installations for each version. We created a new project called [chrome-versions](https://github.com/ulixee/chrome-versions) that downloads, extracts, and stores versions of Chrome for Linux, Windows and Mac. \n\nFor each version, we stripped out the auto-update features and converted them to .tar archives that can be extracted side-by-side. They're then published on Github as release assets for each Chrome version (eg, https://github.com/ulixee/chrome-versions/releases/tag/88.0.4324.182)\n\nOn Debian/Ubuntu, Chrome often needs packages to be installed. We re-bundled the .deb control file into a new installer that can be run after you extract the chrome executable - this makes setting up CI or docker very simple for 1 or more Chrome installations.\n\n## Ulixee Hero\nHero has been updated to use Chrome everywhere. Our emulators have \"polyfills\" auto-generated for how to resemble Chrome headed when running each version headless. The changes are significant enough from Chromium that you need to actually use Chrome underneath. \n\nNo changes should be visible in your scripts, but you might see some installation changes as you go to upgrade. We also experienced some changes in no-sandbox features when running on Docker. Your mileage here may vary. \n\nThis new release will have an updated Dockerfile and files under `tools/docker/*` showing how to get up and running on Docker-slim.\n",
            "url": "https://ulixee.org/blog/chromium-chrome",
            "title": "Moving from Chromium to Chrome",
            "date_modified": "2021-02-19T00:00:00.000Z"
        },
        {
            "id": "https://ulixee.org/blog/handling-scale",
            "content_html": "\nWhen you start using Hero, you often copy and paste the default examples. As we started to use Hero on larger extraction efforts, it became clear that we didn't have a clear story for \"how\" you go from that starting example to running 2, or even 1000 scrapes.\n\nAs you start to think about structuring a bigger effort, a bunch of questions come up:\n\n- Do you create a new Hero instance every time? Or do you simply add tabs?\n- How expensive is it to create many instances?\n- How should I make sure not to overload the host machine with the number of scrapes running at the same time?\n- How do I add new machines when I max out the current one?\n\nAs we explored simplifying this story, we wanted to make the progression of \"examples\" through to full-scrapes a smooth process. Something like this:\n\n#### Step 1: Try Out an Example\n\nTrying out examples should require as little setup as possible, so we added a new `default export` that's a ready-to-go client for Hero.\n\n```js\nimport hero from `@ulixee/hero`;\n\n(async () => {\n  // no initilization required!\n  await hero.goto('https://ulixee.org');\n  const datasetLinks = await hero.document.querySelectorAll('a.DatasetSummary');\n  for (const link of datasetLinks) {\n    const name = await link.querySelector('.title').textContent;\n    const href = await link.getAttribute('href');\n    const dataset = { name, href };\n    console.log('Ulixee Dataset', dataset);\n  }\n\n  await hero.close();\n})();\n```\n\n#### Step 2: Run Multiple Scrapes\n\nHero instances are lightweight, but what do you do when you need to queue up thousands of them to run. Until now, you've been on your own to use libraries like `p-queue`, keeping track of promises, or simply waiting and looping.\n\nWe introduced a new idea into Hero called a [`Handler`](/docs/basic-interfaces/handler) to help run multiple scrapes in one session. Handlers manage the concurrency of multiple scrapes to ensure your machine doesn't get overloaded and hang. We designed it so your code should require almost no changes to transition to many scrapes.\n\n```js\nimport { Handler } from `@ulixee/hero`;\n\n(async () => {\n  const handler = new Handler({ maxConcurrency: 5 });\n\n  handler.dispatchHero(async hero => {\n    // hero is automatically created for us\n    await hero.goto('https://ulixee.org');\n    const datasetLinks = await hero.document.querySelectorAll('a.DatasetSummary');\n    for (const link of datasetLinks) {\n      const name = await link.querySelector('.title').textContent;\n      const href = await link.getAttribute('href');\n      const dataset = { name, href };\n\n      // add a name to each hero so we can find each scrape on Replay\n      const heroOptions = { name };\n      handler.dispatchHero(getDatasetCost, dataset, heroOptions);\n    }\n  });\n\n  // only 5 heros will be active at a given time until all are done\n  await handler.waitForAllDispatches();\n  await handler.close();\n})();\n\n// my data gets passed in once an hero is available\nasync function getDatasetCost(hero, dataset) {\n  let { name, href } = dataset;\n  if (!href.startsWith('http')) href = `https://ulixee.org${href}`;\n  console.log(href);\n  await hero.goto(href);\n  await hero.waitForPaintingStable();\n  const cost = await hero.document.querySelector('.cost .large-text').textContent;\n  console.log('Cost of %s is %s', dataset.name, cost);\n}\n```\n\n#### Step 3: Add Scraping Machines\n\nYou might find that you need to increase the speed of your scrapes. So the next transition you'll likely want to make is to add remote machines. Handlers are built to round-robin between multiple [`ConnectionToCore`](/docs/advanced/connection-to-core) instances.\n\n```js\nimport { Handler } from `@ulixee/hero`;\n\n(async () => {\n  const handler = new Handler(\n    {\n      maxConcurrency: 5,\n      host: '192.168.1.1:2300', // fictional remote hero #1\n    },\n    {\n      maxConcurrency: 5,\n      host: '192.168.1.2:2300', // fictional remote hero #2\n    },\n  );\n  \n// ... everything else is the same!\n\n  handler.dispatchHero(async hero => {\n    // hero is automatically created for us\n    await hero.goto('https://ulixee.org');\n    ...\n```\n\n\n#### Default Exports\n\nTo get to this setup, you'll notice some changes in the default exports when you install Hero 1.3.0-alpha.1. The default exports that come out of the `@ulixee/hero` package is now a pre-initialized instance of the `Hero` class.\n\n[`Handler`](/docs/basic-interfaces/handler) and [`Hero`](/docs/basic-interfaces/hero) are available as exports from `@ulixee/hero` if you'd like to continue to use those. To customize a \"Remote\" `Hero` for an [`Hero`](/docs/basic-interfaces/hero), you can create a new instance with a [`connectionToCore`](/docs/basic-interfaces/hero#constructor) parameter, or use the [`.configure()`](/docs/basic-interfaces/hero#configure) function.\n\n\n#### That's it!\n\nThat's our change. We hope it leads to a very simple model to understand how to scale up your Hero instances. Feedback is welcome as always on any of our channels (listed in header)!\n",
            "url": "https://ulixee.org/blog/handling-scale",
            "title": "Scaling Hero Scrapes with Handlers",
            "date_modified": "2020-12-29T00:00:00.000Z"
        }
    ]
}